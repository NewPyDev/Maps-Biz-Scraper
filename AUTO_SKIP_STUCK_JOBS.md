# Auto-Skip Stuck Jobs Feature âœ…

## What Was Added

The scraper now **automatically detects and skips stuck jobs** to prevent wasting time on problematic searches.

### 1. Job Timeout Protection

**30-minute maximum per job**
- If a job runs longer than 30 minutes, it automatically fails and moves to the next job
- Prevents jobs from running for hours like "Plumbers in New York" did

```
â° Job timeout: 35 minutes elapsed (max 30 minutes)
```

### 2. Stuck Detection

**10-minute progress check**
- If no businesses are saved for 10 minutes, the job is marked as stuck
- Automatically skips to next job instead of hanging forever

```
âš ï¸ Job appears stuck: No progress for 10 minutes
```

### 3. Browser Cleanup Between Jobs

**Each job gets a fresh start:**
1. âœ… Browser closes after each job (success or failure)
2. âœ… New proxy selected for next job
3. âœ… Fresh Chrome instance with clean state
4. âœ… Prevents memory leaks and crashes

```
ğŸ”’ Closing browser...
âœ“ Browser closed successfully
ğŸ”„ Ready for next job with fresh proxy
```

### 4. Progress Tracking

The scraper now tracks:
- **Job start time** - when the job began
- **Last progress time** - when the last business was saved
- **Elapsed time** - total time spent on current job

## How It Works

### Normal Job Flow
```
1. Job starts â†’ Timer starts
2. Search Google Maps â†’ Find businesses
3. Extract business #1 â†’ Progress timer resets âœ“
4. Extract business #2 â†’ Progress timer resets âœ“
5. Extract business #3 â†’ Progress timer resets âœ“
...
50. Job completes â†’ Browser closes â†’ New proxy â†’ Next job
```

### Stuck Job Flow
```
1. Job starts â†’ Timer starts
2. Search Google Maps â†’ Find 0 businesses
3. Wait 10 minutes... (no progress)
4. âš ï¸ STUCK DETECTED â†’ Skip to next job
5. Browser closes â†’ New proxy â†’ Next job
```

### Timeout Job Flow
```
1. Job starts â†’ Timer starts
2. Extract businesses slowly...
3. 30 minutes pass...
4. â° TIMEOUT â†’ Skip to next job
5. Browser closes â†’ New proxy â†’ Next job
```

## Settings

You can adjust these in `scraper_with_database.py`:

```python
MAX_JOB_TIME = 1800      # 30 minutes (1800 seconds)
STUCK_THRESHOLD = 600    # 10 minutes (600 seconds)
```

### Recommended Settings

**For fast cities (New York, London, Paris):**
- MAX_JOB_TIME = 1800 (30 minutes)
- STUCK_THRESHOLD = 600 (10 minutes)

**For slow/small cities:**
- MAX_JOB_TIME = 2400 (40 minutes)
- STUCK_THRESHOLD = 900 (15 minutes)

**For testing:**
- MAX_JOB_TIME = 300 (5 minutes)
- STUCK_THRESHOLD = 180 (3 minutes)

## Benefits

### Before (Old Behavior)
âŒ Jobs could run for hours
âŒ Stuck jobs blocked the queue
âŒ Browser crashes from memory leaks
âŒ Manual intervention required
âŒ Wasted proxy bandwidth

### After (New Behavior)
âœ… Jobs auto-skip after 30 minutes
âœ… Stuck jobs detected in 10 minutes
âœ… Fresh browser for each job
âœ… Automatic recovery
âœ… Efficient proxy usage

## Example Logs

### Successful Job
```
INFO: Starting job #5: Plumbers in Madrid, Spain
INFO: ğŸŒ Using proxy: 123.45.67.89:8080
INFO: Found 50 businesses, starting extraction...
INFO: [1/50] Saved: ABC Plumbing Services
INFO: [2/50] Saved: XYZ Plumbers
...
INFO: âœ“ Job #5 completed: 50 businesses saved (45 with websites)
INFO: ğŸ”’ Closing browser...
INFO: âœ“ Browser closed successfully
INFO: ğŸ”„ Ready for next job with fresh proxy
```

### Stuck Job (Auto-Skipped)
```
INFO: Starting job #4: Plumbers in New York, USA
INFO: ğŸŒ Using proxy: 123.45.67.89:8080
INFO: Found 0 businesses, starting extraction...
WARNING: âš ï¸ Job appears stuck: No progress for 10 minutes
ERROR: Job #4 failed: Job stuck: No progress for 10 minutes
INFO: ğŸ”’ Closing browser...
INFO: âœ“ Browser closed successfully
INFO: ğŸ”„ Ready for next job with fresh proxy
```

### Timeout Job (Auto-Skipped)
```
INFO: Starting job #6: Dentists in Tokyo, Japan
INFO: ğŸŒ Using proxy: 123.45.67.89:8080
INFO: Found 200 businesses, starting extraction...
INFO: [1/200] Saved: Tokyo Dental Clinic
...
WARNING: â° Job timeout: 31 minutes elapsed (max 30 minutes)
ERROR: Job #6 failed: Job timeout after 31 minutes
INFO: ğŸ”’ Closing browser...
INFO: âœ“ Browser closed successfully
INFO: ğŸ”„ Ready for next job with fresh proxy
```

## What Happens to Failed Jobs?

Failed jobs are marked as **"failed"** in the database with the error message:
- "Job stuck: No progress for 10 minutes"
- "Job timeout after 30 minutes"

You can:
1. **Delete them** from the dashboard (they're probably bad searches)
2. **Reset them** to pending and try again later
3. **Ignore them** and focus on successful jobs

## Testing

To test the stuck detection with a short timeout:

1. Edit `scraper_with_database.py`:
```python
MAX_JOB_TIME = 300      # 5 minutes
STUCK_THRESHOLD = 180   # 3 minutes
```

2. Add a job that will fail (e.g., "Test in NonexistentCity")
3. Watch it auto-skip after 3 minutes

## Dashboard Integration

The dashboard will show:
- âœ… **Completed jobs** - finished successfully
- âŒ **Failed jobs** - stuck or timeout
- â³ **Pending jobs** - waiting to run
- ğŸ”„ **Running jobs** - currently scraping

Failed jobs won't block the queue anymore!

---

**Status**: Active and protecting your scraping queue! ğŸ›¡ï¸
